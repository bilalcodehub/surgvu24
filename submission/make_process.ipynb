{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b907982-3366-43e7-aea3-42a36dbae17a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#|default_exp process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1eab7f8c-a1cf-4538-9567-4a279ee252b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from fastai.vision.all import *\n",
    "import SimpleITK\n",
    "from pandas import DataFrame\n",
    "from scipy.ndimage import center_of_mass, label\n",
    "from pathlib import Path\n",
    "from evalutils import ClassificationAlgorithm\n",
    "from evalutils.validators import (\n",
    "    UniquePathIndicesValidator,\n",
    "    DataFrameValidator,\n",
    ")\n",
    "from typing import (Tuple)\n",
    "from evalutils.exceptions import ValidationError\n",
    "import random\n",
    "from typing import Dict\n",
    "import json\n",
    "import subprocess\n",
    "import shutil\n",
    "import logging\n",
    "from skimage.measure import label, regionprops, find_contours\n",
    "import segmentation_models_pytorch as smp\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f625f0d2-bb0a-440f-b247-eda5ddc80725",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# Set up logging configuration\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c222800-2428-40bb-8347-4388de421e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "execute_in_docker = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ce9401-2eb2-418b-a4be-bfa93aed183c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def get_image(r): \n",
    "    \"\"\"\n",
    "    Constructs the full image path for a given row in the DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        r (pd.Series): A row from the DataFrame.\n",
    "    \n",
    "    Returns:\n",
    "        str: Full path to the image file.\n",
    "    \"\"\"\n",
    "    return str(dataroot_path / 'frames' / f\"{r['filename']}.jpg\")\n",
    "\n",
    "def get_label(r): \n",
    "    \"\"\"\n",
    "    Retrieves the task label from a given row in the DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        r (pd.Series): A row from the DataFrame.\n",
    "    \n",
    "    Returns:\n",
    "        str: The task label.\n",
    "    \"\"\"\n",
    "    return r['task_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79013607-1796-431b-8df3-66b6b4d590f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class VideoLoader():\n",
    "    def load(self, *, fname):\n",
    "        path = Path(fname)\n",
    "        if not path.is_file():\n",
    "            raise IOError(\n",
    "                f\"Could not load {fname} using {self.__class__.__qualname__}.\"\n",
    "            )\n",
    "            #cap = cv2.VideoCapture(str(fname))\n",
    "        #return [{\"video\": cap, \"path\": fname}]\n",
    "        return [{\"path\": fname}]\n",
    "\n",
    "# only path valid\n",
    "    def hash_video(self, input_video):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c18d6a-8352-4721-bae4-515146b654e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class UniqueVideoValidator(DataFrameValidator):\n",
    "    \"\"\"\n",
    "    Validates that each video in the set is unique\n",
    "    \"\"\"\n",
    "\n",
    "    def validate(self, *, df: DataFrame):\n",
    "        try:\n",
    "            hashes = df[\"video\"]\n",
    "        except KeyError:\n",
    "            raise ValidationError(\"Column `video` not found in DataFrame.\")\n",
    "\n",
    "        if len(set(hashes)) != len(hashes):\n",
    "            raise ValidationError(\n",
    "                \"The videos are not unique, please submit a unique video for \"\n",
    "                \"each case.\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a8e24b-aaa9-48c8-a497-ee6586f67342",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class SurgVU_classify(ClassificationAlgorithm):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            index_key='input_video',\n",
    "            file_loaders={'input_video': VideoLoader()},\n",
    "            input_path=Path(\"/input/\") if execute_in_docker else Path(\"./test/\"),\n",
    "            output_file=Path(\"/output/surgical-step-classification.json\") if execute_in_docker else Path(\n",
    "                \"./output/surgical-step-classification.json\"),\n",
    "            validators=dict(\n",
    "                input_video=(\n",
    "                    # UniqueVideoValidator(),\n",
    "                    UniquePathIndicesValidator(),\n",
    "                )\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # Log the initialization process\n",
    "        logger.info('Initializing the model and setting up configurations.')\n",
    "\n",
    "        # Set CPU flag\n",
    "        self.cpu = False\n",
    "        self.window_size = 8\n",
    "        models_path = Path('/opt/algorithm/models/') if execute_in_docker else Path(\"./models/\")\n",
    "\n",
    "        # Load the task models\n",
    "        try:\n",
    "            logger.info(f'Loading models from the {models_path} directory.')\n",
    "            self.learners = [load_learner(m, cpu=self.cpu) for m in models_path.ls() if m.suffix == '.pkl']\n",
    "            logger.info(f'{len(self.learners)} models are located & successfully loaded.')\n",
    "        except Exception as e:\n",
    "            logger.error(f'Error loading models: {e}')\n",
    "            raise\n",
    "\n",
    "        # Verify that all learners have the same vocabulary\n",
    "        try:\n",
    "            logger.info('Verifying that all learners have the same vocabulary.')\n",
    "            # Retrieve the vocab from each learner\n",
    "            vocabs = [learner.dls.vocab for learner in self.learners]\n",
    "            # Use the vocab from the first learner as reference\n",
    "            reference_vocab = vocabs[0]\n",
    "            for idx, vocab in enumerate(vocabs[1:], start=1):\n",
    "                if vocab != reference_vocab:\n",
    "                    logger.error(f'Vocabulary mismatch between learner 0 and learner {idx}')\n",
    "                    # Optionally, you can log the differences\n",
    "                    diff = set(vocab) ^ set(reference_vocab)\n",
    "                    logger.error(f'Differences in vocabularies: {diff}')\n",
    "                    raise ValueError('Learner vocabularies do not match!')\n",
    "            logger.info('All learners have the same vocabulary.')\n",
    "            self.vocab = reference_vocab  # Store the common vocabulary\n",
    "        except Exception as e:\n",
    "            logger.error(f'Error during vocabulary verification: {e}')\n",
    "            raise\n",
    "\n",
    "        # Initialize the surgical step list\n",
    "        self.step_list = [\n",
    "            \"range_of_motion\",\n",
    "            \"rectal_artery_vein\",\n",
    "            \"retraction_collision_avoidance\",\n",
    "            \"skills_application\",\n",
    "            \"suspensory_ligaments\",\n",
    "            \"suturing\",\n",
    "            \"uterine_horn\",\n",
    "            \"other\"\n",
    "        ]\n",
    "\n",
    "        logger.info('Surgical step list initialized successfully.')\n",
    "        \n",
    "        # Verify that step_list and vocab contain the same elements\n",
    "        try:\n",
    "            if set(self.step_list) != set(self.vocab):\n",
    "                logger.error('Mismatch between elements in step_list and learners\\' vocabulary.')\n",
    "                missing_in_vocab = set(self.step_list) - set(self.vocab)\n",
    "                missing_in_step_list = set(self.vocab) - set(self.step_list)\n",
    "                if missing_in_vocab:\n",
    "                    logger.error(f'Elements in step_list not in vocab: {missing_in_vocab}')\n",
    "                if missing_in_step_list:\n",
    "                    logger.error(f'Elements in vocab not in step_list: {missing_in_step_list}')\n",
    "                raise ValueError('step_list does not contain the same elements as learners\\' vocabulary.')\n",
    "            else:\n",
    "                logger.info('step_list contains the same elements as the learners\\' vocabulary.')\n",
    "        except Exception as e:\n",
    "            logger.error(f'Error during step_list verification: {e}')\n",
    "            raise\n",
    "\n",
    "\n",
    "    def dummy_step_prediction_model(self):\n",
    "        random_step_prediction = random.randint(0, len(self.step_list)-1)\n",
    "        return random_step_prediction\n",
    "    \n",
    "    \n",
    "    def step_predict_json_sample(self):\n",
    "        single_output_dict = {\"frame_nr\": 1, \"surgical_step\": None}\n",
    "        return single_output_dict\n",
    "\n",
    "\n",
    "    def process_case(self, *, idx, case):\n",
    "\n",
    "        # Input video would return the collection of all frames (cap object)\n",
    "        input_video_file_path = case #VideoLoader.load(case)\n",
    "        # Detect and score candidates\n",
    "        scored_candidates = self.predict(case.path) #video file > load evalutils.py\n",
    "\n",
    "        # return\n",
    "        # Write resulting candidates to result.json for this case\n",
    "        return scored_candidates\n",
    "\n",
    "\n",
    "    def save(self):\n",
    "        try:\n",
    "            with open(str(self._output_file), \"w\") as f:\n",
    "                json.dump(self._case_results[0], f)\n",
    "            logger.info(f'Predictions saved to {str(self._output_file)} successfully.')\n",
    "        except Exception as e:\n",
    "            logger.error(f'Failed to save predictions to {str(self._output_file)}. Error: {e}')\n",
    "            raise\n",
    "\n",
    "    def split_video(self, video_path: Path):\n",
    "        # Ensure the input is a Path object and the file exists\n",
    "        if not isinstance(video_path, Path):\n",
    "            raise TypeError(\"video_path must be a Path object.\")\n",
    "        if not video_path.is_file():\n",
    "            raise FileNotFoundError(f\"The file {video_path} does not exist.\")\n",
    "\n",
    "        # Create the frames directory in the parent of the parent directory\n",
    "        frames_dir = video_path.parent.parent / \"frames\" / video_path.stem\n",
    "\n",
    "        # Delete the output directory if it already exists\n",
    "        if frames_dir.exists() and frames_dir.is_dir():\n",
    "            shutil.rmtree(frames_dir)\n",
    "\n",
    "        # Create the output directory\n",
    "        frames_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Prepare the ffmpeg command\n",
    "        output_pattern = frames_dir / \"%d.jpg\"\n",
    "        ffmpeg_command = [\n",
    "            \"ffmpeg\",\n",
    "            \"-i\", str(video_path),\n",
    "            \"-start_number\", \"0\",     # Start numbering from 0\n",
    "            \"-r\", \"1\",\n",
    "            str(output_pattern)\n",
    "        ]\n",
    "\n",
    "        # Run the ffmpeg command\n",
    "        subprocess.run(ffmpeg_command, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=True)\n",
    "\n",
    "        return frames_dir\n",
    "    \n",
    "\n",
    "    def get_sorted_image_files(self, frames_dir: Path):\n",
    "        # Get all image files in the directory\n",
    "        image_files = get_image_files(frames_dir)\n",
    "\n",
    "        # Sort files based on the integer value of the filename\n",
    "        sorted_files = sorted(image_files, key=lambda f: int(''.join(filter(str.isdigit, f.stem))))\n",
    "\n",
    "        return L(sorted_files)\n",
    "\n",
    "    def smooth_predictions(self, predicted_classes: List[str], window_size: int = 1) -> List[str]:\n",
    "        \"\"\"\n",
    "        Smooths the predicted class labels by exploring the neighboring frames.\n",
    "\n",
    "        Args:\n",
    "            predicted_classes (List[str]): The list of predicted class labels per frame.\n",
    "            window_size (int): Number of neighboring frames to consider on each side for smoothing.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: The smoothed list of predicted class labels.\n",
    "        \"\"\"\n",
    "        if not predicted_classes:\n",
    "            return []\n",
    "\n",
    "        num_frames = len(predicted_classes)\n",
    "        smoothed_predictions = []\n",
    "\n",
    "        for i in range(num_frames):\n",
    "            # Define the window boundaries\n",
    "            start_idx = max(0, i - window_size)\n",
    "            end_idx = min(num_frames - 1, i + window_size)\n",
    "\n",
    "            # Extract the neighborhood labels\n",
    "            neighborhood = predicted_classes[start_idx:end_idx + 1]\n",
    "\n",
    "            # Count the frequency of each label in the neighborhood\n",
    "            label_counts = {}\n",
    "            for label in neighborhood:\n",
    "                label_counts[label] = label_counts.get(label, 0) + 1\n",
    "\n",
    "            # Identify the most common label(s)\n",
    "            max_count = max(label_counts.values())\n",
    "            common_labels = [label for label, count in label_counts.items() if count == max_count]\n",
    "\n",
    "            # Resolve ties by retaining the current frame's label\n",
    "            if len(common_labels) == 1:\n",
    "                most_common_label = common_labels[0]\n",
    "            else:\n",
    "                most_common_label = predicted_classes[i]\n",
    "\n",
    "            smoothed_predictions.append(most_common_label)\n",
    "\n",
    "        return smoothed_predictions\n",
    "\n",
    "    def predict(self, fname) -> Dict:\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        fname -> video file path\n",
    "\n",
    "        Output:\n",
    "        tools -> list of prediction dictionaries (per frame) in the correct format as described in documentation \n",
    "        \"\"\"\n",
    "        logger.info(f'Starting prediction process for video: {str(fname)}')\n",
    "\n",
    "        # Step 1: Split the video into frames\n",
    "        frames_dir = self.split_video(fname)\n",
    "        logger.info(f'Frame extraction completed for video: {str(fname)}. Frames stored in directory: {frames_dir}')\n",
    "\n",
    "        # Step 2: Get sorted list of image files\n",
    "        image_files = self.get_sorted_image_files(frames_dir)\n",
    "        image_files = [str(image_file) for image_file in image_files]\n",
    "        num_frames = len(image_files)\n",
    "        logger.info(f'{num_frames} frames identified for processing.')\n",
    "\n",
    "        # Step 3: Perform inference with ensemble\n",
    "        logger.info('Performing inference with ensemble models...')\n",
    "        all_raw_preds = []\n",
    "        test_items = []\n",
    "\n",
    "        # Collect predictions from each model in the ensemble\n",
    "        for idx, learner in enumerate(self.learners):\n",
    "            # Create a test_dl to apply the requisite transforms\n",
    "            test_dl = learner.dls.test_dl(image_files, bs=64, num_workers=0)\n",
    "\n",
    "            # For the first learner, store test_items\n",
    "            if idx == 0:\n",
    "                test_items = [str(item) for item in test_dl.items]\n",
    "                logger.info('Stored test items from the first learner for validation.')\n",
    "            else:\n",
    "                # For other learners, compare test_dl.items to test_items\n",
    "                current_items = [str(item) for item in test_dl.items]\n",
    "                if current_items != test_items:\n",
    "                    logger.error(f\"Test items in learner {idx} do not match the first learner.\")\n",
    "                    # Optionally, log the differences\n",
    "                    differences = [(i, item1, item2) for i, (item1, item2) in enumerate(zip(current_items, test_items)) if item1 != item2]\n",
    "                    logger.error(f\"Differences found in test items: {differences}\")\n",
    "                    raise ValueError(f\"Test items in learner {idx} do not match the first learner.\")\n",
    "                else:\n",
    "                    logger.info(f'Test items in learner {idx} match the first learner.')\n",
    "\n",
    "            # Get predictions from the current learner\n",
    "            raw_preds, _ = learner.get_preds(dl=test_dl)\n",
    "            all_raw_preds.append(raw_preds)\n",
    "\n",
    "        # Average the predictions across all models\n",
    "        avg_raw_preds = torch.stack(all_raw_preds).mean(0)\n",
    "        logger.info('Inference completed successfully using ensemble models.')\n",
    "\n",
    "        # Step 4: Decode predictions to class names\n",
    "        softmax_preds = avg_raw_preds.softmax(dim=1)\n",
    "        argmax_preds = softmax_preds.argmax(dim=1)\n",
    "        predicted_classes = [self.vocab[pred] for pred in argmax_preds]\n",
    "\n",
    "        # --- Added Step: Smooth Predictions ---\n",
    "        logger.info('Applying smoothing to predictions...')\n",
    "        # Assuming `smooth_predictions_with_neighbors` is a method of the class\n",
    "        smoothed_pred_classes = self.smooth_predictions(predicted_classes, window_size=self.window_size)\n",
    "        logger.info('Smoothing applied successfully.')\n",
    "        # --- End of Added Step ---\n",
    "\n",
    "        # Step 5: Generate output JSON\n",
    "        logger.info('Formatting predictions for JSON output...')\n",
    "        # Create a mapping from class names to indices in step_list\n",
    "        step_list_index_map = {name: idx for idx, name in enumerate(self.step_list)}\n",
    "        all_frames_predicted_outputs = []\n",
    "        for img_path, class_name in zip(test_items, smoothed_pred_classes):  # Use smoothed_pred_classes instead of predicted_classes\n",
    "            img_path = Path(img_path)\n",
    "            frame_dict = self.step_predict_json_sample()\n",
    "            frame_dict['frame_nr'] = int(img_path.stem)\n",
    "\n",
    "            # Map the predicted class name to the corresponding index in self.step_list\n",
    "            frame_dict[\"surgical_step\"] = step_list_index_map.get(class_name, -1)\n",
    "\n",
    "            all_frames_predicted_outputs.append(frame_dict)\n",
    "\n",
    "        steps = all_frames_predicted_outputs\n",
    "        logger.info(f'Prediction process completed for video: {str(fname)}. Results ready for output.')\n",
    "\n",
    "        return steps\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    SurgVU_classify().process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37e96ef4-8ae1-4471-9b19-a589239ba65d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nbdev.export import nb_export\n",
    "nb_export('make_process.ipynb', '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5738e8b0-6593-4836-a49d-e9f26b72227d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
